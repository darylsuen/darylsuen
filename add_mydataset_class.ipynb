{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "import pandas as pd\n",
    "import xarray\n",
    "\n",
    "from neuralhydrology.datasetzoo.basedataset import BaseDataset\n",
    "from neuralhydrology.utils.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NovaScotiaBasins(BaseDataset):\n",
    "    \"\"\"Template class for adding a new data set.\n",
    "    \n",
    "    Each dataset class has to derive from `BaseDataset`, which implements most of the logic for preprocessing data and \n",
    "    preparing data for model training. Only two methods have to be implemented for each specific dataset class: \n",
    "    `_load_basin_data()`, which loads the time series data for a single basin, and `_load_attributes()`, which loads \n",
    "    the static attributes for the specific data set. \n",
    "    \n",
    "    Usually, we outsource the functions to load the time series and attribute data into separate functions (in the\n",
    "    same file), which we then call from the corresponding class methods. This way, we can also use specific basin data\n",
    "    or dataset attributes without these classes.\n",
    "    \n",
    "    To make this dataset available for model training, don't forget to add it to the `get_dataset()` function in \n",
    "    'neuralhydrology.datasetzoo.__init__.py'\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : Config\n",
    "        The run configuration.\n",
    "    is_train : bool \n",
    "        Defines if the dataset is used for training or evaluating. If True (training), means/stds for each feature\n",
    "        are computed and stored to the run directory. If one-hot encoding is used, the mapping for the one-hot encoding \n",
    "        is created and also stored to disk. If False, a `scaler` input is expected and similarly the `id_to_int` input\n",
    "        if one-hot encoding is used. \n",
    "    period : {'train', 'validation', 'test'}\n",
    "        Defines the period for which the data will be loaded\n",
    "    basin : str, optional\n",
    "        If passed, the data for only this basin will be loaded. Otherwise the basin(s) are read from the appropriate\n",
    "        basin file, corresponding to the `period`.\n",
    "    additional_features : List[Dict[str, pd.DataFrame]], optional\n",
    "        List of dictionaries, mapping from a basin id to a pandas DataFrame. This DataFrame will be added to the data\n",
    "        loaded from the dataset, and all columns are available as 'dynamic_inputs', 'evolving_attributes' and\n",
    "        'target_variables'\n",
    "    id_to_int : Dict[str, int], optional\n",
    "        If the config argument 'use_basin_id_encoding' is True in the config and period is either 'validation' or \n",
    "        'test', this input is required. It is a dictionary, mapping from basin id to an integer (the one-hot encoding).\n",
    "    scaler : Dict[str, Union[pd.Series, xarray.DataArray]], optional\n",
    "        If period is either 'validation' or 'test', this input is required. It contains the centering and scaling\n",
    "        for each feature and is stored to the run directory during training (train_data/train_data_scaler.yml).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 cfg: Config,\n",
    "                 is_train: bool,\n",
    "                 period: str,\n",
    "                 basin: str = None,\n",
    "                 additional_features: List[Dict[str, pd.DataFrame]] = [],\n",
    "                 id_to_int: Dict[str, int] = {},\n",
    "                 scaler: Dict[str, Union[pd.Series, xarray.DataArray]] = {}):\n",
    "        # initialize parent class\n",
    "        super(NovaScotiaBasins, self).__init__(cfg=cfg,\n",
    "                                              is_train=is_train,\n",
    "                                              period=period,\n",
    "                                              basin=basin,\n",
    "                                              additional_features=additional_features,\n",
    "                                              id_to_int=id_to_int,\n",
    "                                              scaler=scaler)\n",
    "\n",
    "    def _load_basin_data(self, basin: str) -> pd.DataFrame:\n",
    "        \"\"\"Load timeseries data for a single basin.\n",
    "        \n",
    "        This function is used to load the time series data (meteorological \n",
    "        forcing, streamflow, etc.) and make available as time series input for \n",
    "        model training later on. Make sure that the returned dataframe is \n",
    "        time-indexed.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        basin : str\n",
    "            Basin identifier as string.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Time-indexed DataFrame, containing the time series data (e.g., \n",
    "            forcings + discharge).\n",
    "        \"\"\"\n",
    "\n",
    "        return load_ns_rdrs_timeseries(self.cfg.data_dir, basin)\n",
    "\n",
    "    def _load_attributes(self) -> pd.DataFrame:\n",
    "        \"\"\"Load basin static variables for specified basins in a list given as\n",
    "        an argument.\n",
    "        \n",
    "        This function is used to load basin attribute data (e.g. CAMELS \n",
    "        catchments attributes) as a basin-indexed dataframe with features \n",
    "        in columns.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame of .csv file containing the static attributes for the\n",
    "        specified basins in the dataset. If no basins are specified, all\n",
    "        basins are returned.\n",
    "        \"\"\"\n",
    "\n",
    "        return load_basin_attributes(self.cfg.data_dir, self.basins)\n",
    "\n",
    "\n",
    "def load_ns_rdrs_timeseries(data_dir: Path, basin: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the time series data for a specific basin.\n",
    "    \n",
    "    Arguments:\n",
    "        data_dir: path to the root directory called 'LSTM_NS_Data'.\n",
    "        \n",
    "        basin: the name of the .csv file containing the time series data for \n",
    "        the basin which should be the gauge station id eg. '01FJ002'.\n",
    "        \n",
    "        Returns: the .csv file as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    preprocessed_dir = data_dir / \"PreprocessedTimeseries\"\n",
    "\n",
    "    # Make sure the data was already preprocessed and per-basin files exist.\n",
    "    if not preprocessed_dir.is_dir():\n",
    "        msg = [\n",
    "            f\"No preprocessed data directory found at {preprocessed_dir}. Use\" \n",
    "            \"preprocessed_camels_cl_dataset ,\"\n",
    "            \"in neuralhydrology.datasetzoo.camelscl to preprocess the CAMELS\" \n",
    "            \"CL data set once into ,\"\n",
    "            \"per-basin files.\"\n",
    "        ]\n",
    "        raise FileNotFoundError(\"\".join(msg))\n",
    "\n",
    "    # Load the data for the specific basin into a time-indexed dataframe.\n",
    "    basin_file = preprocessed_dir / f\"{basin}.csv\"\n",
    "    df = pd.read_csv(basin_file, index_col='date', parse_dates=['date'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_basin_attributes(data_dir: Path, basins: List[str] = []) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the static attributes for all basins in the dataset.\n",
    "    \n",
    "    Arguments: \n",
    "        data_dir: path to the root directory called 'LSTM_NS_Data'.\n",
    "        \n",
    "        basins: a list of basin identifiers for which to load the attributes. \n",
    "        If empty, all basins will be loaded.\n",
    "        \n",
    "    Returns: a pandas DataFrame of the basin static variables for the specified\n",
    "    basins.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Load attributes into basin-indexed dataframe.\n",
    "    attributes_file = data_dir / 'basin_attributes.csv'\n",
    "    df = pd.read_csv(attributes_file, index_col=\"basin_id\").transpose()\n",
    "\n",
    "    # Convert all columns, where possible, to numeric.\n",
    "    df = df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "    # Convert the two columns specifying record period start and end to \n",
    "    # datetime format\n",
    "    df[\"record_period_start\"] = pd.to_datetime(df[\"record_period_start\"])\n",
    "    df[\"record_period_end\"] = pd.to_datetime(df[\"record_period_end\"])\n",
    "\n",
    "    if basins:\n",
    "        if any(b not in df.index for b in basins):\n",
    "            raise ValueError('Some basins are missing static attributes.')\n",
    "        # Filter the dataframe to only include the specified basins.\n",
    "        df = df.loc[basins]\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "NovaScotiaBasins.__init__() missing 2 required positional arguments: 'is_train' and 'period'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mNovaScotiaBasins\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBaseDataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: NovaScotiaBasins.__init__() missing 2 required positional arguments: 'is_train' and 'period'"
     ]
    }
   ],
   "source": [
    "NovaScotiaBasins(BaseDataset, is_train=True, period=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neuralhydrology.datasetzoo.camelsus import NovaScotiaBasins\n",
    "from neuralhydrology.datasetzoo.camelscl import CamelsCL\n",
    "from neuralhydrology.datasetzoo.camelsgb import CamelsGB\n",
    "from neuralhydrology.datasetzoo.camelsus import CamelsUS\n",
    "from neuralhydrology.datasetzoo.hourlycamelsus import HourlyCamelsUS\n",
    "\n",
    "def get_dataset(cfg: Config,\n",
    "                is_train: bool,\n",
    "                period: str,\n",
    "                basin: str = None,\n",
    "                additional_features: list = [],\n",
    "                id_to_int: dict = {},\n",
    "                scaler: dict = {}) -> BaseDataset:\n",
    "    \"\"\"Get data set instance, depending on the run configuration.\n",
    "\n",
    "    Arguments:\n",
    "        See documentation at: \n",
    "        https://neuralhydrology.readthedocs.io/en/latest/api/neuralhydrology.\n",
    "        datasetzoo.html#neuralhydrology.datasetzoo.get_dataset\n",
    "\n",
    "    Currently implemented datasets are 'caravan', 'camels_aus', 'camels_br', \n",
    "    'camels_cl', 'camels_gb', 'camels_us', and 'hourly_camels_us', as well as \n",
    "    the 'generic' dataset class, which we have added one called \n",
    "    'novascotia_basin', that can be used for any kind of dataset as long as it \n",
    "    is in the correct format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check config argument and select appropriate data set class.\n",
    "    if cfg.dataset == \"novascotia_basins\":\n",
    "        Dataset = NovaScotiaBasins\n",
    "    if cfg.dataset == \"camels_us\":\n",
    "        Dataset = CamelsUS\n",
    "    elif cfg.dataset == \"camels_gb\":\n",
    "        Dataset = CamelsGB\n",
    "    elif cfg.dataset == \"hourly_camels_us\":\n",
    "        Dataset = HourlyCamelsUS\n",
    "    elif cfg.dataset == \"camels_cl\":\n",
    "        Dataset = CamelsCL\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"No dataset class implemented for dataset {cfg.dataset}\"\n",
    "            )\n",
    "\n",
    "    # initialize dataset\n",
    "    ds = Dataset(cfg=cfg,\n",
    "                 is_train=is_train,\n",
    "                 period=period,\n",
    "                 basin=basin,\n",
    "                 additional_features=additional_features,\n",
    "                 id_to_int=id_to_int,\n",
    "                 scaler=scaler)\n",
    "    return ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuralHydrology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
